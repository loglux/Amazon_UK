# LLM configuration (choose one backend)
# Ollama example:
# LLM_BACKEND=ollama
# LLM_ENDPOINT=http://192.168.10.229:11434
# LLM_MODEL=llama3.2
# LLM_TIMEOUT=300

# OpenAI/DeepSeek/OpenRouter style:
# LLM_BACKEND=openai
# LLM_ENDPOINT=https://api.deepseek.com
# LLM_MODEL=deepseek-chat
# LLM_API_KEY=YOUR_KEY_HERE
# LLM_TIMEOUT=120

# Override API port mappings if needed
# API_BASE=http://localhost:8000
